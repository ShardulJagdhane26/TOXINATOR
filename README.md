# TOXINATOR âš¡

Check if comments are rude, harmful, or hateful in seconds.

TOXINATOR uses a BiLSTM deep learning model and Gemini commentary to analyze how toxic a comment is. It returns overall toxicity, perâ€‘label probabilities, and a short AI "hot take" explaining the tone.

---

## ğŸš€ Live Demo

[https://toxinator.vercel.app](https://toxinator.vercel.app)

---

## ğŸ§  What It Does

- Multiâ€‘label toxicity classification on 6 categories:
  - `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, `identity_hate`
- Overall toxicity score + perâ€‘label probabilities
- AI commentary (Gemini) that explains the tone in a short, witty paragraph

### Model Highlights

- **Architecture:** Embedding â†’ BiLSTM â†’ Dense layers (multiâ€‘label sigmoid)
- **Parameters:** ~6.49M
- **Precision on heldâ€‘out data:** ~92.8%

---

## Project Structure

```
TOXINATOR/
â”œâ”€ backend/
â”‚  â”œâ”€ app.py            # Flask API + TensorFlow model + Gemini proxy
â”‚  â”œâ”€ toxicity.h5       # Trained BiLSTM toxicity model (Keras H5)
â”‚  â”œâ”€ vocab.json        # Vocabulary exported from TextVectorization
â”‚  â”œâ”€ requirements.txt  # Python runtime + TensorFlow
â”‚  â””â”€ .env.example      # Environment variable template
â”œâ”€ frontend/
â”‚  â””â”€ index.html        # Singleâ€‘page cartoon UI (Tailwind + vanilla JS)
â””â”€ notebook/
   â””â”€ ...               # Training / exploration notebooks
```

---

## ğŸ”§ Backend (Flask + TensorFlow)

### 1. Environment Setup

```bash
cd backend
python -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Run API Locally

```bash
cd backend
python app.py
```

By default it starts on `http://localhost:5000`.

### API Endpoints

- `GET /api/health` â€“ service + model status
- `POST /api/classify` â€“ classify one comment

### Request Body

```json
{
  "comment": "You freaking suck! I am going to hit you."
}
```

### Sample Response

```json
{
  "success": true,
  "comment": "You freaking suck! I am going to hit you.",
  "toxic_score": 0.73,
  "is_toxic": true,
  "percentage": 73.0,
  "message": "Toxic! âš ï¸",
  "detailed_scores": [0.81, 0.12, 0.64, 0.03, 0.77, 0.09],
  "categories": ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"],
  "analysis_text": "Short Gemini commentary goes here..."
}
```

---

## ğŸ“Š Model Performance

| Metric | Value |
|--------|-------|
| Architecture | Embedding â†’ BiLSTM â†’ Dense Layers |
| Total Parameters | ~6.49M |
| Precision | ~92.8% |
| Output Type | Multi-label sigmoid |

---

## ğŸ“ Important Notes

- The toxicity model is trained on the **Jigsaw toxic comment dataset**; like all ML models, it can make mistakes and may not generalize to every domain.
- **Please do not rely on it as a single source of truth for moderation decisions in sensitive or highâ€‘stakes environments.**
- Model metrics can be updated if you decide to retrain or optimize the model later.

---